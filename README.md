# Twitter ETL Pipeline

This project demonstrates the extraction of data from Twitter using the Twitter API, processing the data with Apache Airflow, and storing the results in an AWS S3 bucket. The pipeline is deployed on an AWS EC2 Ubuntu instance.

## Prerequisites

- AWS account
- Twitter Developer account and access to Twitter API
- Local machine with SSH capability to access AWS EC2

## Setup

### 1. Twitter API Access

- Obtain Twitter API credentials (API key, API secret key, Access token, and Access token secret).
- Store these credentials securely on your local system. They will be needed for configuring the Twitter data extraction script.

### 2. AWS EC2 Ubuntu Instance

- Create an AWS EC2 Ubuntu instance.
- Allow inbound traffic for necessary ports (e.g., 8080 for Airflow) in the EC2 security group settings.

### 3. Apache Airflow Deployment

- SSH into your EC2 instance.
- Execute commands from `twitter_commands.sh` to install Apache Airflow and other necessary dependencies on the EC2 instance.

### 4. Airflow Configuration

- Start Airflow using the `airflow standalone` command.
- Note the username and password generated by Airflow during the setup.
- Access the Airflow web interface by navigating to `http://[your-ec2-instance-public-ip]:8080` in a web browser.

### 5. Airflow DAG

- Place the `twitter_dag.py` file in the Airflow dags directory.
- This DAG is responsible for orchestrating the Twitter data extraction, processing, and loading into S3.

### 6. AWS S3 Bucket

- Create an AWS S3 bucket to store the output files generated by the Airflow job.
- Ensure the EC2 instance has an IAM role with permissions to write to the S3 bucket.

## Execution

1. **Start Airflow Scheduler and Webserver:**
   - Ensure the Airflow scheduler and webserver are running by executing the `airflow standalone` command if not already running.

2. **Launch Airflow Web UI:**
   - Open a web browser and access `http://[your-ec2-instance-public-ip]:8080` to view the Airflow dashboard.

3. **Trigger the DAG:**
   - Locate the `twitter_dag` in the Airflow dashboard.
   - Trigger the DAG manually for the first run or wait for its scheduled execution based on the DAG configuration.

4. **Monitor the Pipeline:**
   - Use the Airflow web interface to monitor the pipeline execution and troubleshoot any issues that arise.

5. **Verify Data in S3:**
   - Upon successful execution of the DAG, check the specified S3 bucket to ensure that the data files have been written.

## Security Notes

- Always secure your API keys and sensitive information.
- Ensure that your EC2 instance is not overly exposed to the internet by limiting inbound traffic to only necessary ports.

## Conclusion

This README outlines the process for setting up and executing a Twitter ETL pipeline using Apache Airflow on an AWS EC2 instance, with data storage in AWS S3. Adjust configurations and security settings as necessary to fit your specific requirements.